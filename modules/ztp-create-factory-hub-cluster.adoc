// Module included in the following assemblies:
//
// * scalability_and_performance/ztp-factory-install-clusters.adoc
:_content-type: CONCEPT
[id="preparing-the-factory-install-environment_{context}"]
= Preparing the factory install environment

[discrete]
=== Base prerequisites

* Deploy the {product-title} cluster with three control plane nodes following the guidance in the section "Deploying installer-provisioned clusters on bare metal". Alternatively you can use the technology preview Assisted Installer from link:https://cloud.redhat.com/[cloud.redhat.com] to create the cluster. To install single-node OpenShift follow the guidance in "Installing on a single node" in the {product-title} documentation.
** All cluster Operators are available.
** Cluster is reachable using a `KUBECONFIG` file.
** The API, API-INT and ingress should be deployed on edge cluster on the DHCP external network.

[discrete]
=== Storage prerequisites

* Storage can be provided by installing the Local Storage Operator and by using local volumes or by using OpenShift Data Foundation (ODF). Deploy ODF following the guidance in link:https://access.redhat.com/documentation/en-us/red_hat_openshift_data_foundation/4.9/html/deploying_openshift_data_foundation_using_bare_metal_infrastructure/index[Deploying OpenShift Data Foundation using bare metal infrastructure].
+
[NOTE]
====
If the cluster is greater than 3 nodes, the recommendation is to use OpenShift Data Foundation. If it is a single-node OpenShift cluster, use the Local Storage Operator.
====

* Create the following persistent volumes with at least 200GB of storage (NVMe or SSD) for:

    ** 2 for Assisted Installer.
    ** 1 for the hub internal registry that is for the mirror of the images. At least 200GB is required on the hub, more may be required if ODF is installed.
    ** 1 for HTTPD that hosts the Red Hat Enterprise Linux CoreOS (RHCOS) images.
    ** 1 for zero touch provisioning factory workflows (ZTPFW).
    ** 1 for Red Hat Advanced Cluster Manager (RHACM)

[discrete]
=== Networking prerequisites

The hub cluster requires internet connectivity and should be installed on a private network with customer configured DNS and DHCP services. Configure DNS for the API on the ingress of the hub to reach some routes on the hub cluster. Configure enough DNS entries for the number of edge clusters you intend to deploy in parallel.

You need enough DHCP addresses to host the number of edge clusters you intend to deploy. Each {product-title} node in the cluster must have access to an NTP server. {product-title} nodes use NTP to synchronize their clocks. For example, cluster nodes use SSL certificates that require validation, which might fail if the date and time between the nodes are not in sync.

Specific requirements are:

* DNS entries need to be configured and resolvable from the external network, with DNS on the DHCP external network.
* Hub
** `api.<hub-clustername>.<baseDomain>` and `api-int.<hub-clustername>.<baseDomain>` entries to the same IP address.
** ingress (`*.apps.<hub-clustername>.<baseDomain>`).

* Edge
** `api.<spoke-clustername>.<baseDomain>` and `api-int.<spoke-clustername>.<baseDomain>` entries to the same IP address.
** ingress (`*.apps.<spoke-clustername>.<baseDomain>`).

* External DHCP with enough free IPs on the factory network to provide access to the edge cluster by using the external network interface.

* Every edge cluster needs at least 6 IPs from this external network (without the broadcast and network IP).
** 1 per node.
** 1 for API.
** 1 for API-INT.
** 1 for the Ingress entry (`*.apps.<spoke-clustername>.<baseDomain>`).
