// Module included in the following assemblies:
//
// networking/multiple_networks/about-chaining.adoc

:_mod-docs-content-type: PROCEDURE
[id="configuring-plugin-chaining-with-multus-cni_{context}"]
= Configuring plugin chaining with Multus CNI

Plugin chaining allows you to configure multiple CNI plugins to be applied sequentially to the same network interface. This approach is useful when you need to implement complex network configurations, such as routing traffic over different network paths or creating isolated networks for specialized traffic types.

For instance, in a telco environment, you might have the following requirements:

* SIP traffic (Voice over IP) must always be routed over a dedicated SIP network for optimized performance.
* Management traffic must go over a separate management network for administrative purposes.

Consider a telco application where SIP (telephony) traffic must be isolated from other types of traffic for example management or data traffic. This requires two networks:

* SIP Network: Handles all telephony traffic to ensure quality of service (QoS) and low latency.
* Management Network: Handles administrative traffic such as monitoring and configuration.

In this case, you can configure a pod with two interfaces:

* `eth1` for management traffic, routed through the management network.
* `eth2` for SIP traffic, routed through the SIP network.

Following this example you can use plugin chaining with the `route-override` CNI, to attach both interfaces to the pod and use source-based routing to ensure SIP traffic is only routed through `eth2` while all other traffic flows over `eth1`.

.Prerequisites

* Install the OpenShift CLI (`oc`).
* An account with `cluster-admin` privileges.

.Procedure

. Create the `telco-app` namespace by running the following command:
+
[source,terminal]
----
$ oc create namespace telco-app
----

. Create NetworkAttachmentDefinition for Management Network

.. Create a YAML file, such as `management.yaml`, to define a NetworkAttachmentDefinition (NAD) that configures a new interface, `eth1`, with the following configuration:
+
[source,yaml]
----
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  name: management-net
  namespace: telco-app
spec:
  config: '{
    "cniVersion": "1.0.0",
    "name": "management-net",
    "plugins": [
      {
        "type": "macvlan",
        "master": "br-ex",
	"vlan": 100,
        "mode": "bridge",
        "ipam": {
          "type": "static",
          "addresses": [
            {
              "address": "192.168.100.10/24",
              "gateway": "192.168.100.1"
            }
          ]
        }
      },
      {
        "type": "route-override",
        "table": 100,
        "srcRules": [
          {
            "from": "192.168.100.10/32",
            "table": 100
          }
        ],
        "routes": [
          { "dst": "0.0.0.0/0", "gw": "192.168.100.1" }
        ]
      }
    ]
  }'
----

. Create a chained NAD by running the following command:
+
[source,terminal]
----
$ oc apply -f management.yaml
----

. Create NetworkAttachmentDefinition for SIP Network.

.. Create a YAML file, such as `sip.yaml`, to define a NetworkAttachmentDefinition (NAD) that configures a new interface, `eth2`, with the following configuration:
+
[source,yaml]
----
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  name: sip-net
  namespace: telco-app
spec:
  config: '{
    "cniVersion": "1.0.0",
    "name": "sip-net",
    "plugins": [
      {
        "type": "macvlan",
        "master": "br-ex",
        "vlan": 200,
        "mode": "bridge",
        "ipam": {
          "type": "static",
          "addresses": [
            {
              "address": "192.168.200.10/24",
              "gateway": "192.168.200.1"
            }
          ]
        }
      },
      {
        "type": "route-override",
        "table": 200,
        "srcRules": [
          {
            "from": "192.168.200.10/32",
            "table": 200
          }
        ],
        "routes": [
          { "dst": "0.0.0.0/0", "gw": "192.168.200.1" }
        ]
      }
    ]
  }'
----

. Create the chained NAD by running the following command:
+
[source,terminal]
----
$ oc apply -f sip.yaml
----

.  Attach the NADs to a pod by creating a Pod definition file, such as `pod.yaml`, with the following configuration:
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: telco-app-pod
  namespace: telco-app
  labels:
    app: telco-app
  annotations:
    k8s.v1.cni.cncf.io/networks: '[
      { "name": "management-net", "interface": "eth1" },
      { "name": "sip-net", "interface": "eth2" }
    ]'
spec:
  securityContext:
    runAsNonRoot: true
    seccompProfile:
      type: RuntimeDefault
  containers:
  - name: app-container
    image: centos/tools
    command: ["sleep", "infinity"]
    securityContext:
      allowPrivilegeEscalation: false
      capabilities:
        drop: ["ALL"]
      runAsUser: 1000
      runAsGroup: 1000
----

. Create the pod by running the following command:
+
[source,terminal]
----
$ oc apply -f pod.yaml
----

.Verification

. Run the following command to list all network interfaces and their assigned IP addresses inside the `telco-app-pod`. This verifies that the pod has multiple network interfaces configured as expected:
+
[source,terminal]
----
$ oc exec -it telco-app-pod -n telco-app -- ip a
----

. Run the following command to inspect the IP routing rules inside the `telco-app-pod`. This displays the source-based routing rules applied within the pod to ensure SIP traffic is routed through `eth2`:
+
[source,terminal]
----
$ oc exec -it telco-app-pod -n telco-app -- ip rule show
----
+
.Example output
[source,terminal]
----
1: lo: <LOOPBACK,UP,LOWER_UP> mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000
    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00
    inet 127.0.0.1/8 scope host lo
       valid_lft forever preferred_lft forever
    inet6 ::1/128 scope host 
       valid_lft forever preferred_lft forever
2: eth0@if21: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 8901 qdisc noqueue state UP group default 
    link/ether 0a:58:0a:83:00:11 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 10.131.0.17/23 brd 10.131.1.255 scope global eth0
       valid_lft forever preferred_lft forever
    inet6 fe80::858:aff:fe83:11/64 scope link 
       valid_lft forever preferred_lft forever
3: eth1@if5: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9001 qdisc noqueue state UP group default qlen 1000
    link/ether 86:6b:14:37:cb:24 brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 192.168.100.10/24 brd 192.168.100.255 scope global eth1
       valid_lft forever preferred_lft forever
    inet6 fe80::846b:14ff:fe37:cb24/64 scope link 
       valid_lft forever preferred_lft forever
4: eth2@if5: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 9001 qdisc noqueue state UP group default qlen 1000
    link/ether da:a7:46:a2:0b:cd brd ff:ff:ff:ff:ff:ff link-netnsid 0
    inet 192.168.200.10/24 brd 192.168.200.255 scope global eth2
       valid_lft forever preferred_lft forever
    inet6 fe80::d8a7:46ff:fea2:bcd/64 scope link 
       valid_lft forever preferred_lft forever
----

This output show the pod is attached connected to multiple networks, enabling it to interact with different external services or data planes.

. Run the following command to view the routing table. 
+
[source,terminal]
----
$ oc exec -it telco-app-pod -n telco-app -- ip route
----
+
.Example output
[source,terminal]
----
default via 10.131.0.1 dev eth0 
10.128.0.0/14 via 10.131.0.1 dev eth0 
10.131.0.0/23 dev eth0 proto kernel scope link src 10.131.0.17 
100.64.0.0/16 via 10.131.0.1 dev eth0 
169.254.0.5 via 10.131.0.1 dev eth0 
172.30.0.0/16 via 10.131.0.1 dev eth0 
192.168.100.0/24 dev eth1 proto kernel scope link src 192.168.100.10 
192.168.200.0/24 dev eth2 proto kernel scope link src 192.168.200.10
----

This output shows the routing table inside the pod, which includes the default route and routes for the management and SIP networks. 

* SIP Network Traffic (eth2)
** The pod has an interface `eth2` with an IP address `192.168.200.10` on the `192.168.200.0/24` network. Traffic destined for the `192.168.200.0/24` network will be routed through `eth2`, as expected for the SIP traffic.

* Management Network Traffic (eth1)
** The pod has an interface `eth1` with an IP address `192.168.100.10` on the `192.168.100.0/24` network. Traffic destined for 192.168.100.0/24 (management traffic) will be routed through eth1, which meets the requirement for handling administrative traffic.