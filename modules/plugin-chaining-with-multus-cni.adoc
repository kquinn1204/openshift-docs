// Module included in the following assemblies:
//
// networking/multiple_networks/about-chaining.adoc

:_mod-docs-content-type: PROCEDURE
[id="configuring-plugin-chaining-with-multus-cni_{context}"]
= Configuring plugin chaining with Multus CNI

Plugin chaining allows you to configure multiple CNI plugins to be applied sequentially to the same network interface. This approach is useful when you need to implement complex network configurations, such as routing traffic over different network paths or creating isolated networks for specialized traffic types.

For instance, in a telco environment, you may have the following requirements:

* SIP traffic (Voice over IP) must always be routed over a dedicated SIP network for optimized performance.
* Management traffic must go over a separate management network for administrative purposes.

Consider a telco application where SIP (telephony) traffic must be isolated from other types of traffic for example management or data traffic. This requires two networks:

* SIP Network: Handles all telephony traffic to ensure quality of service (QoS) and low latency.
* Management Network: Handles administrative traffic such as monitoring and configuration.

In this case, you can configure a pod with two interfaces:

* `eth1` for management traffic, routed through the management network.
* `eth2` for SIP traffic, routed through the SIP network.

Following this example you can use plugin chaining with Multus CNI, to attach both interfaces to the pod and use source-based routing to ensure SIP traffic is only routed through `eth2` while all other traffic flows over eth1.

.Prerequisites

* Install the OpenShift CLI (`oc`).
* An account with `cluster-admin` privileges.

.Procedure

. Create a YAML file, such as `chained.yaml`, to define a chained NetworkAttachmentDefinition (NAD) that configures two additional interfaces, `eth1` and `eth2`, with the following configuration:
+
[source,yaml]
----
apiVersion: k8s.cni.cncf.io/v1
kind: NetworkAttachmentDefinition
metadata:
  name: chained-net
  namespace: telco-app
spec:
  config: '{
    "cniVersion": "0.4.0",
    "name": "chained-net",
    "plugins": [
      {
        "type": "macvlan", <1>
        "capabilities": { "ips": true },
        "master": "eth1", <2>
        "mode": "bridge",
        "ipam": {
          "type": "static",
          "addresses": [
            {
              "address": "192.168.100.10/24", <3>
              "gateway": "192.168.100.1" <4>
            }
          ]
        }
      },
      {
        "type": "macvlan",<5>
        "capabilities": { "ips": true },
        "master": "eth2", <6>
        "mode": "bridge",
        "ipam": {
          "type": "static",
          "addresses": [
            {
              "address": "10.0.0.10/24", <7>
              "gateway": "10.0.0.1" <8>
            }
          ]
        }
      },
      {
        "type": "route-override", <9>
        "table": 100,
        "routes": [
          { "dst": "0.0.0.0/0", "gw": "192.168.100.1" }
        ]
      },
      {
        "type": "static",
        "routes": [
          { "dst": "192.168.100.0/24", "gw": "192.168.100.1" },
          { "dst": "10.0.0.0/24", "gw": "10.0.0.1", "table": 200 }
        ],
        "rules": [
          { "priority": 100, "src": "10.0.0.10/32", "table": 200 } <8>
        ]
      }
    ]
  }'
----

<1> The first plugin is a `macvlan` plugin that creates a new interface for example the management interface `eth1` with a static IP address.
<2> The `master` field specifies the parent interface to attach the new interface to.
<3> Pods will be assigned an IP from the 192.168.100.0/24 range.
<4> The `gateway` field specifies the gateway IP address for the new interface.
<5> The second plugin is a `macvlan` plugin that creates a new interface for example the SIP interface `eth2` with a static IP address.
<6> The `master` field specifies the parent interface to attach the new interface to.
<7> Pods will be assigned an IP from the 10.0.0.0/24 range.
<8> The `gateway` field specifies the gateway IP address for the new interface.
<9> The third plugin is a `route-override` plugin that sets the default route for the pod to use the `eth1` interface. The `route-override` plugin ensures non-SIP traffic is routed via the management network.

.  Attach the NAD to a pod by creating a Pod definition file, such as `pod.yaml`, with the following configuration:
+
[source,yaml]
----
apiVersion: v1
kind: Pod
metadata:
  name: telco-app-pod
  namespace: telco-app
  annotations:
    k8s.v1.cni.cncf.io/networks: '[{ "name": "chained-net" }]'
spec:
  containers:
  - name: app-container
    image: centos/tools
    command: ["sleep", "infinity"]
----

.Verification

* Check that the Operator is installed by entering the following command:
+
[source,terminal]
----
$ oc get csv -n openshift-sriov-network-operator \
  -o custom-columns=Name:.metadata.name,Phase:.status.phase
----
+
.Example output
[source,terminal,subs="attributes+"]
----
Name                                         Phase
sriov-network-operator.{product-version}.0-202406131906   Succeeded
----

[id="install-operator-web-console_{context}"]
== Web console: Installing the SR-IOV Network Operator

As a cluster administrator, you can install the Operator using the web console.

.Prerequisites

* A cluster installed on bare-metal hardware with nodes that have hardware that supports SR-IOV.
* Install the OpenShift CLI (`oc`).
* An account with `cluster-admin` privileges.

.Procedure


. Install the SR-IOV Network Operator:

.. In the {product-title} web console, click *Operators* -> *OperatorHub*.

.. Select *SR-IOV Network Operator* from the list of available Operators, and then click *Install*.

.. On the *Install Operator* page, under *Installed Namespace*, select *Operator recommended Namespace*.

.. Click *Install*.

. Verify that the SR-IOV Network Operator is installed successfully:

.. Navigate to the *Operators* -> *Installed Operators* page.

.. Ensure that *SR-IOV Network Operator* is listed in the *openshift-sriov-network-operator* project with a *Status* of *InstallSucceeded*.
+
[NOTE]
====
During installation an Operator might display a *Failed* status.
If the installation later succeeds with an *InstallSucceeded* message, you can ignore the *Failed* message.
====

+
If the Operator does not appear as installed, to troubleshoot further:

+
* Inspect the *Operator Subscriptions* and *Install Plans* tabs for any failure or errors under *Status*.
* Navigate to the *Workloads* -> *Pods* page and check the logs for pods in the `openshift-sriov-network-operator` project.
* Check the namespace of the YAML file. If the annotation is missing, you can add the annotation `workload.openshift.io/allowed=management` to the Operator namespace with the following command:
+
[source,terminal]
----
$ oc annotate ns/openshift-sriov-network-operator workload.openshift.io/allowed=management
----
+
[NOTE]
====
For {sno} clusters, the annotation `workload.openshift.io/allowed=management` is required for the namespace.
====
